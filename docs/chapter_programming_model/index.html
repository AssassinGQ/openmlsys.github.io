<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>2. 编程接口 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. 计算图" href="../chapter_computational_graph/index.html" />
    <link rel="prev" title="1. 导论" href="../chapter_introduction/index.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active"><span class="section-number">2. </span>编程接口</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_programming_model/index.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2. 编程接口</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2. 编程接口</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">2. </span>编程接口<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>现代机器学习框架包含大量的组件。这些组件使得用户得以高效开发机器学习算法，处理数据，部署模型，性能调优和使用硬件加速器。在设计这些组件的编程接口时，一个核心的诉求是：如何平衡框架性能和易用性？为了达到最优的性能，开发者需要利用硬件亲和的编程语言如：C和C++来进行开发。这是因为：C和C++的使用使得机器学习框架可以高效硬件的底层API，从而最大限度发挥硬件。同时，现代操作系统（如Linux和Windows）提供丰富的基于C和C++的编程接口（如文件系统，网络编程，多线程管理等），通过直接调用操作系统API，可以降低框架运行的开销。</p>
<p>从易用性的角度分析，机器学习框架的使用者往往具有丰富的行业背景（如数据科学家，生物学家，化学家，物理学家等）。他们的常用的编程语言是高层次脚本语言：Python，Matlab，R和Julia。相比于C和C++，这些语言在提供编程的易用性的同时，丧失了C和C++对底层硬件和操作系统进行深度优化的能力。因此，机器学习框架的核心设计目标是：其要具有易用编程接口来支持用户用高层次语言如Python来实现机器学习算法，同时其也要具备以C和C++为核心的低层次编程接口，使得框架开发者可以用C和C++实现大量高性能组件，从而在硬件上高效执行。在本章中，我们将会讲述如何达到这个设计目标。</p>
<p>本章的学习目标包括：</p>
<ul class="simple">
<li><p>理解机器学习系统的工作流和以Python为核心的编程接口设计。</p></li>
<li><p>理解机器学习系统以神经网络模块为核心的接口设计原理和实现。</p></li>
<li><p>理解机器学习系统的底层C/C++执行算子的实现和与上层Python接口的调用实现。</p></li>
<li><p>了解机器学习系统编程接口的演进方向。</p></li>
</ul>
<div class="section" id="id2">
<h2><span class="section-number">2.1. </span>机器学习系统编程模型的演进<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="id18">
<span id="img-framedh"></span><a class="reference internal image-reference" href="../_images/framework_development_history.png"><img alt="../_images/framework_development_history.png" src="../_images/framework_development_history.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.1.1 </span><span class="caption-text">机器学习编程库发展历程</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>随着机器学习通的诞生，如何设计易用且高性能的编程接口就一直成为了框架设计者首要解决的问题。在早期的机器学习框架中（如图:numref:<cite>img_framedh</cite>所示），人们选择用Lua（Torch）和Python（Theano）等高层次编程语言来编写机器学习程序。这些早期的机器学习框架提供了机器学习必须的模型定义，自动微分等功能，其适用于编写小型和科研为导向的机器学习应用。</p>
<p>在2011年，深度神经网络快速崛起，并很快在各个AI应用领域（计算机视觉，语音识别，自然语言处理等）取得了最先进的性能。训练深度神经网络需要消耗大量的算力，而这些算力无法被以Lua和Python所主导开发的Torch和Theano所满足。与此同时，计算加速卡（如英伟达GPU）的通用编程接口（例如CUDA
C）日趋成熟，而构建于CPU多核技术之上的多线程库（POSIX
Threads）也被广大开发者所接受。因此，许多的机器学习用户希望基于C和C++来开发高性能的深度学习应用。这一类需求被Caffe等一系列以C和C++作为核心编程接口的框架所满足。</p>
<p>然而，机器学习模型往往需要针对部署场景，数据类型，识别任务等需求进行深度定制，而这类定制任务需要被广大的AI应用领域的开发者所实现。这类开发者的背景多样，其往往不具有熟练使用C和C++的背景，因此Caffe这一类库与C和C++深度绑定的编程模型快速成为了制约这一类框架快速推广的巨大瓶颈。</p>
<p>在2016年，谷歌率先推出了TensorFlow。相比于传统的Caffe，Torch和Theano，TensorFlow提出利用高层次编程语言：Python作为面向用户的主要前端语言，而利用C和C++实现高性能后端。大量基于Python的前端API确保了TensorFlow可以被大量的数据科学家和机器学习科学家接受，同时帮助TensorFlow能够快速融入Python为主导的大数据生态（大量的大数据开发库如Numpy，Pandas，Scrapy,
Matplotlib和PySpark）。同时，Python具有出色的和C语言的互操作性，这种互操作性已经在多个Python库中得到验证。因此，TensorFlow兼有Python的灵活性和生态，同时也通过C/C++后端得以实现高性能。这种设计在日后崛起的PyTorch，MXNet和CNTK的机器学习框架得到传承。</p>
<p>随着多个机器学习框架的出现，Keras和TensorLayer等高层次机器学习开发库提供了更高层次的Python
API从而可以快速导入已有的模型，
这些高层次API进一步屏蔽了底层框架的实现细节，因此Keras和TensorLayer可以运行在不同的机器学习框架之上。</p>
<p>随着深度神经网络的进一步发展，对于机器学习框架编程接口的挑战也日益增长。因此在2020年前后，新型的机器学习框架如MindSpore和JAX进一步出现。其中，MindSpore在继承了TensorFlow，PyTorch的Python和C/C++的混合接口的基础上，进一步拓展了机器学习编程模型从而可以高效支持多种AI后端芯片（如华为Ascend，英伟达GPU和ARM芯片），实现了机器学习应用在海量异构设备上的快速部署。</p>
<p>同时，超大型数据集和超大型深度神经网络崛起让分布式执行成为了机器学习框架编程模型的核心设计需求。为了实现分布式执行，TensorFlow和PyTorch的使用者需要进行大量编程来将数据集和神经网络分配到分布式节点上，而大量的AI开发人员并不具有分布式编程的能力。因此MindSpore进一步完善了机器学习框架的分布式编程模型的能力，从而让单节点的MindSpore程序可以无缝地运行在海量节点上。</p>
<p>在本小节中，我们将以MindSpore作为例子讲解一个现代机器学习框架的Python前端API和C/C++后端API的设计原则。这些设计原则和PyTorch，TensorFlow相似。</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">2.2. </span>机器学习工作流<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>机器学习系统编程模型的首要设计目标是：对开发者的整个工作流进行完整的编程支持。一个常见的机器学习任务一般包含如图：numref:<code class="docutils literal notranslate"><span class="pre">img_workflow</span></code>所示的流程。这个工作流完成了训练数据集的读取，模型的训练，测试和调试。通过归纳，我们可以将这一工作流中用户所需要自定义的部分通过定义以下API来支持（我们这里假设用户的高层次API以Python函数的形式提供）：</p>
<ul class="simple">
<li><p><strong>数据处理：</strong>
首先，用户需要数据处理API来支持将数据集从磁盘读入。进一步，用户需要对读取数据进行数据预处理，从而可以将数据输入后续的机器学习模型中。</p></li>
<li><p><strong>模型定义：</strong>
完成数据的读取后，用户需要模型定义API来定义机器学习模型。这些模型带有模型参数，可以对给定的数据进行推理。</p></li>
<li><p><strong>损失函数和优化算法：</strong>
模型的输出需要和用户的标记进行对比，这个对比差异一般通过损失函数（Loss
function）来进行评估。因此，优化器定义API允许用户定义自己的损失函数，并且根据损失来引入（Import）和定义各种优化算法（Optimisation
algorithms）来计算梯度（Gradient），完成对模型参数的更新。</p></li>
<li><p><strong>训练过程：</strong>
给定一个数据集，模型，损失函数和优化器，用户需要训练API来定一个循环（Loop）从而将数据集中的数据按照小批量（mini-batch）的方式读取出来，反复计算梯度来更新模型。这个反复的过程称为训练。</p></li>
<li><p><strong>测试和调优：</strong>
训练过程中，用户需要测试API来对当前模型的精度进行评估。当精度达到目标后，训练结束。这一过程中，用户往往需要调试API来完成对模型的性能和正确性进行验证。</p></li>
</ul>
<div class="figure align-default" id="id19">
<span id="img-workflow"></span><a class="reference internal image-reference" href="../_images/workflow.png"><img alt="../_images/workflow.png" src="../_images/workflow.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.2.1 </span><span class="caption-text">机器学习系统工作流</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">2.2.1. </span>环境配置<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>在构建机器学习工作流程前，MindSpore需要通过context.set_context来配置运行需要的信息，如运行模式、后端信息、硬件等信息。
导入context模块，配置运行需要的信息。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;MindSpore MLPNet Example&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device_target&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">])</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
</pre></div>
</div>
<p>上述配置样例运行使用图模式。根据实际情况配置硬件信息，譬如代码运行在Ascend
AI处理器上，则–device_target选择Ascend，代码运行在CPU、GPU同理。</p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">2.2.2. </span>数据处理<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>配置好运行信息后，首先讨论数据处理API的设计。这些API提供了大量Python函数支持用户用一行命令即可读入常见的训练数据集（如MNIST，CIFAR，COCO等）。
在加载之前需要下载数据集存放在./datasets/MNIST_Data路径中；MindSpore提供了用于数据处理的API模块
mindspore.dataset，用于存储样本和标签。在加载数据集前，通常会对数据集进行一些处理，mindspore.dataset也集成了常见的数据处理方法。
以下代码读取了MNIST的数据是大小为<span class="math notranslate nohighlight">\(28 \times 28\)</span>的图片，返回DataSet对象。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="n">DATA_DIR</span> <span class="o">=</span> <span class="s1">&#39;./datasets/MNIST_Data/train&#39;</span>
<span class="n">mnist_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">)</span>
</pre></div>
</div>
<p>有了DataSet对象后，通常需要对数据进行增强，常用的数据增强包括翻转、旋转、剪裁、缩放等；在MindSpore中是使用map将数据增强的操作映射到数据集中的，之后进行打乱（Shuffle）和批处理（Batch）。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 导入需要用到的模块</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms.c_transforms</span> <span class="k">as</span> <span class="nn">C</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision.c_transforms</span> <span class="k">as</span> <span class="nn">CV</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.vision</span> <span class="kn">import</span> <span class="n">Inter</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="c1"># 数据处理过程</span>
<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># 定义数据集</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">rescale_nml</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">0.3081</span>
    <span class="n">shift_nml</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="mf">0.1307</span> <span class="o">/</span> <span class="mf">0.3081</span>

    <span class="c1"># 定义所需要操作的map映射</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">Inter</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">)</span>
    <span class="n">rescale_nml_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale_nml</span> <span class="o">*</span> <span class="n">rescale</span><span class="p">,</span> <span class="n">shift_nml</span><span class="p">)</span>
    <span class="n">hwc2chw_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="c1"># 使用map映射函数，将数据操作应用到数据集</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="p">[</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">rescale_nml_op</span><span class="p">,</span><span class="n">hwc2chw_op</span><span class="p">],</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span><span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>

    <span class="c1"># 进行shuffle、batch操作</span>
    <span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mnist_ds</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h3><span class="section-number">2.2.3. </span>模型定义<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>使用MindSpore定义神经网络需要继承mindspore.nn.Cell，神经网络的各层需要预先在__init__方法中定义，然后重载__construct__方法实现神经网络的前向传播过程。
因为输入大小被处理成<span class="math notranslate nohighlight">\(32 \times 32\)</span>的图片，需要用Flatten将数据压平为一维向量后给全连接层，全连接层输入大小为<span class="math notranslate nohighlight">\(32 \times 32\)</span>，预测<span class="math notranslate nohighlight">\(0 \sim 9\)</span>中的哪个数字所以最后输出大小为10，下面定义了一个三层的全连接层。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 导入需要用到的模块</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="c1"># 定义线性模型</span>
<span class="k">class</span> <span class="nc">MLPNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLPNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>
<span class="c1"># 实例化网络</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MLPNet</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h3><span class="section-number">2.2.4. </span>损失函数和优化器<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>有了神经网络组件构建的模型我们还需要定义<strong>损失函数</strong>来计算训练过程中输出和真实值的误差。<strong>均方误差</strong>(Mean
Squared
Error，MSE)是线性回归中常用的，是计算估算值与真实值差值的平方和的平均数。
<strong>平均绝对误差</strong>（Mean Absolute
Error，MAE）是计算估算值与真实值差值的绝对值求和再求平均。
<strong>交叉熵</strong>（Cross
Entropy，CE）是分类问题中常用的，衡量已知数据分布情况下，计算输出分布和已知分布的差值，</p>
<p>有了损失函数，我们就可以通过损失值利用<strong>优化器</strong>对参数进行训练更新。对于优化的目标函数<span class="math notranslate nohighlight">\(f(x)\)</span>；先求解其梯度<span class="math notranslate nohighlight">\(\nabla\)</span><span class="math notranslate nohighlight">\(f(x)\)</span>，然后将训练参数<span class="math notranslate nohighlight">\(W\)</span>沿着梯度的负方向更新，更新公式为：<span class="math notranslate nohighlight">\(W_t = W_{t-1} - \alpha\nabla(W_{t-1})\)</span>，其中<span class="math notranslate nohighlight">\(\alpha\)</span>是学习率，<span class="math notranslate nohighlight">\(W\)</span>是训练参数，<span class="math notranslate nohighlight">\(\alpha\nabla(W_{t-1})\)</span>是方向。
神经网络的优化器种类很多，一类是学习率不受梯度影响的随机梯度下降（Stochastic
Gradient
Descent）及SGD的一些改进方法，如带有Momentum的SGD；另一类是自适应学习率如AdaGrad、RMSProp、Adam等。</p>
<p><strong>SGD</strong>的更新是对每个样本进行梯度下降，因此计算速度很快，但是单样本更新频繁，会造成震荡；为了解决震荡问题，提出了带有Momentum的SGD，该方法的参数更新不仅仅由梯度决定，也和累计的梯度下降方向有关，使得增加更新梯度下降方向不变的维度，减少更新梯度下降方向改变的维度，从而速度更快也减少震荡。</p>
<p>自适应学习率<strong>AdaGrad</strong>是通过以往的梯度自适应更新学习率不同的参数<span class="math notranslate nohighlight">\(W_i\)</span>具有不同的学习率。AdaGrad对频繁变化的参数以更小的步长更新，而稀疏的参数以更大的步长更新。因此对稀疏的数据表现比较好。<strong>Adadelta</strong>是对AdaGrad的改进，解决了AdaGrad优化过程中学习率<span class="math notranslate nohighlight">\(\alpha\)</span>单调减少问题；Adadelta不对过去的梯度平方进行累加，用指数平均的方法计算二阶动量，避免了二阶动量持续累积，导致训练提前结束。<strong>Adam</strong>可以理解为Adadelta和Momentum的结合，对一阶二阶动量均采用指数平均的方法计算。</p>
<p>MindSpore提供了丰富的API来让用户导入损失函数和优化器。在下面的例子中，计算了输入和真实值之间的softmax交叉熵损失，导入Momentum优化器。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 定义损失函数</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="c1"># 定义优化器</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h3><span class="section-number">2.2.5. </span>训练及保存模型<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>MindSpore提供了回调Callback机制，可以在训练过程中执行自定义逻辑，使用框架提供的ModelCheckpoint为例。ModelCheckpoint可以保存网络模型和参数，以便进行后续的Fine-tuning（微调）操作。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 导入模型保存模块</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span>
<span class="c1"># 设置模型保存参数</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">1875</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># 应用模型保存参数</span>
<span class="n">ckpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;checkpoint_lenet&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
</pre></div>
</div>
<p>通过MindSpore提供的model.train接口可以方便地进行网络的训练，LossMonitor可以监控训练过程中loss值的变化。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 导入模型训练需要的库</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Accuracy</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="k">def</span> <span class="nf">train_net</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">epoch_size</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">repeat_size</span><span class="p">,</span> <span class="n">ckpoint_cb</span><span class="p">,</span> <span class="n">sink_mode</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;定义训练的方法&quot;&quot;&quot;</span>
    <span class="c1"># 加载训练数据集</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">,</span> <span class="n">repeat_size</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ckpoint_cb</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">(</span><span class="mi">125</span><span class="p">)],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">sink_mode</span><span class="p">)</span>
</pre></div>
</div>
<p>其中，dataset_sink_mode用于控制数据是否下沉，数据下沉是指数据通过通道直接传送到Device上，可以加快训练速度，dataset_sink_mode为True表示数据下沉，否则为非下沉。</p>
<p>有了数据集、模型、损失函数、优化器后就可以进行训练了，这里把train_epoch设置为1，对数据集进行1个迭代的训练。在train_net和
test_net方法中，我们加载了之前下载的训练数据集，mnist_path是MNIST数据集路径。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_epoch</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mnist_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/MNIST_Data&quot;</span>
<span class="n">dataset_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>
<span class="n">train_net</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_epoch</span><span class="p">,</span> <span class="n">mnist_path</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">,</span> <span class="n">ckpoint</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id9">
<h3><span class="section-number">2.2.6. </span>测试和验证<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>测试是模型运行测试数据集得到的结果，通常在训练过程中，每训练一定的数据量后就会测试一次，以验证模型的泛化能力。MindSpore使用model.eval接口读入测试数据集。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;定义验证的方法&quot;&quot;&quot;</span>
    <span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">))</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
<p>在训练完毕后，参数保存在checkpoint中，可以将训练好的参数加载到模型中进行验证。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">load_checkpoint</span><span class="p">,</span> <span class="n">load_param_into_net</span>
<span class="c1"># 加载已经保存的用于测试的模型</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;checkpoint_lenet-1_1875.ckpt&quot;</span><span class="p">)</span>
<span class="c1"># 加载参数到网络中</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="c1"># 使用函数model.predict预测image对应分类</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id10">
<h2><span class="section-number">2.3. </span>定义深度神经网络<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>在上一节我们使用MindSpore构建了一个多层感知机的网络结构，随着深度神经网络的飞速发展，各种深度神经网络结构层出不穷，但是不管结构如何复杂，神经网络层数量如何增加，构建深度神经网络结构始终遵循最基本的规则：1.承载计算的节点；2.可变化的节点权重（节点权重可训练）；3.允许数据流动的节点连接。因此在机器学习编程库中神经网络是以层为核心，它提供了各类神经网络层基本组件；将神经网络层组件按照网络结构进行堆叠、连接就能构造出神经网络模型。</p>
<div class="section" id="id11">
<h3><span class="section-number">2.3.1. </span>以层为核心定义神经网络<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>神经网络层包含构建机器学习网络结构的基本组件，如计算机视觉领域常用到卷积(Convolution)、池化(Pooling)、全连接(Fully
Connected)；自然语言处理常用到循环神经网络(Recurrent Neural
Network，RNN)；为了加速训练，防止过拟合通常用到批标准化（BatchNorm）、Dropout等。</p>
<p><strong>全连接</strong>是将当前层每个节点都和上一层节点一一连接，本质上是特征空间的线性变换；可以将数据从高维映射到低维，也能从低维映射到高维度。
图numref:<code class="docutils literal notranslate"><span class="pre">fc_layer</span></code>展示了全连接的过程，对输入的n个数据变换到另一个大小为m的特征空间，再从大小为m的特征空间变换到大小为p的特征空间；可见全连接层的参数量巨大，两次变换所需的参数大小为<span class="math notranslate nohighlight">\(n \times m\)</span>和<span class="math notranslate nohighlight">\(m \times p\)</span>。</p>
<div class="figure align-default" id="id20">
<span id="fc-layer"></span><a class="reference internal image-reference" href="../_images/fc_layer_1.png"><img alt="../_images/fc_layer_1.png" src="../_images/fc_layer_1.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.1 </span><span class="caption-text">全连接层</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<p><strong>卷积</strong>操作是卷积神经网络中常用的操作之一，卷积相当于对输入进行滑动滤波。根据卷积核（Kernel）、卷积步长（Stride）、填充（Padding）对输入数据从左到右，从上到下进行滑动，每一次滑动操作是矩阵的乘加运算得到的加权值。
如图numref:<code class="docutils literal notranslate"><span class="pre">conv_comp</span></code>卷积操作主要由输入、卷积核、输出组成输出又被称为特征图（Feature
Map）。</p>
<div class="figure align-default" id="id21">
<span id="conv-comp"></span><a class="reference internal image-reference" href="../_images/conv_component.png"><img alt="../_images/conv_component.png" src="../_images/conv_component.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.2 </span><span class="caption-text">卷积操作的组成</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
<p>卷积的具体运算过程我们通过图numref:<code class="docutils literal notranslate"><span class="pre">single_conv</span></code>进行演示。该图输入为<span class="math notranslate nohighlight">\(4 \times 4\)</span>的矩阵，卷积核大小为<span class="math notranslate nohighlight">\(3 \times 3\)</span>，卷积步长为1，不填充，最终得到的<span class="math notranslate nohighlight">\(2 \times 2\)</span>的输出矩阵。
计算过程为将<span class="math notranslate nohighlight">\(3 \times 3\)</span>的卷积核作用到左上角<span class="math notranslate nohighlight">\(3 \times 3\)</span>大小的输入图上；输出为<span class="math notranslate nohighlight">\(1 \times 1 + 2 \times 0 + 2 \times 1 + 3 \times 0 + 2 \times 1 + 3 \times 0 + 4 \times 1 + 1 \times 0 + 3 \ times 1 = 12\)</span>,
同理对卷积核移动1个步长再次执行相同的计算步骤得到第二个输出为11；当再次移动将出界时结束从左往右，执行从上往下移动1步，再进行从左往右移动；依次操作直到从上往下再移动也出界时，结束整个卷积过程，得到输出结果。我们不难发现相比于全连接，卷积的优势是参数共享（同一个卷积核遍历整个输入图）和参数量小（卷积核大小即是参数量）。</p>
<div class="figure align-default" id="id22">
<span id="single-conv"></span><a class="reference internal image-reference" href="../_images/single_channel_conv.png"><img alt="../_images/single_channel_conv.png" src="../_images/single_channel_conv.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.3 </span><span class="caption-text">卷积的具体运算过程</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
<p>在卷积过程中，如果我们需要对输出矩阵大小进行控制，那么就需要对步长和填充进行设置。还是上面的输入图，如需要得到和输入矩阵大小一样的输出矩阵，步长为1时就需要对上下左右均填充一圈全为0的数。</p>
<p>在上述例子中我们介绍了一个输入一个卷积核的卷积操作。通常情况下我们输入的是彩色图片，有三个输入，这三个输入称为通道（Channel），分别代表红、绿、蓝（RGB）。此时我们执行卷积则为多通道卷积，需要三个卷积核分别对RGB三个通道进行上述卷积过程，之后将结果加起来。
具体如图numref:<code class="docutils literal notranslate"><span class="pre">channels_conv</span></code>描述了一个输入通道为3，输出通道为1，卷积核大小为<span class="math notranslate nohighlight">\(3 \times 3\)</span>，卷积步长为1的多通道卷积过程；需要注意的是，每个通道都有各自的卷积核，同一个通道的卷积核参数共享。如果输出通道为<span class="math notranslate nohighlight">\(out_c\)</span>，输入通道为<span class="math notranslate nohighlight">\(in_c\)</span>，那么需要<span class="math notranslate nohighlight">\(out_c\)</span><span class="math notranslate nohighlight">\(\times\)</span><span class="math notranslate nohighlight">\(in_c\)</span>个卷积核。</p>
<div class="figure align-default" id="id23">
<span id="channels-conv"></span><a class="reference internal image-reference" href="../_images/channels_conv.png"><img alt="../_images/channels_conv.png" src="../_images/channels_conv.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.4 </span><span class="caption-text">多通道卷积</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
<p><strong>池化</strong>是常见的降维操作，有最大池化和平均池化。池化操作和卷积的执行类似，通过池化核、步长、填充决定输出；最大池化是在池化核区域范围内取最大值，平均池化则是在池化核范围内做平均。与卷积不同的是池化核没有训练参数；池化层的填充方式也有所不同，平均池化填充的是0，最大池化填充的是<span class="math notranslate nohighlight">\(-inf\)</span>。
图numref:<code class="docutils literal notranslate"><span class="pre">pooling</span></code>是对<span class="math notranslate nohighlight">\(4 \times 4\)</span>的输入进行<span class="math notranslate nohighlight">\(2 \times 2\)</span>区域池化，步长为2，不填充；图左边是最大池化的结果，右边是平均池化的结果。</p>
<div class="figure align-default" id="id24">
<span id="pooling"></span><a class="reference internal image-reference" href="../_images/pooling.png"><img alt="../_images/pooling.png" src="../_images/pooling.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.5 </span><span class="caption-text">池化操作</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>有了卷积、池化、全连接组件就可以构建一个非常简单的卷积神经网络了，图numref:<code class="docutils literal notranslate"><span class="pre">nn_network</span></code>展示了一个卷积神经网络的模型结构。
给定输入<span class="math notranslate nohighlight">\(3 \times 64 \times 64\)</span>的彩色图片，使用16个$3
<a href="#id12"><span class="problematic" id="id13">:raw-latex:`\times 3`$3大小的卷积核做卷积，得到大小为\ :math:`16 \times 64 \times 64`</span></a>；
再进行池化操作降维，得到大小为<span class="math notranslate nohighlight">\(16 \times 32 \times 32\)</span>的特征图；
对特征图再卷积得到大小为<span class="math notranslate nohighlight">\(32 \times 32 \times 32\)</span>特征图，再进行池化操作得到<span class="math notranslate nohighlight">\(3 \times 16 \times 16\)</span>大小的特征图；
我们需要对特征图做全连接，此时需要把特征图平铺成一维向量这部操作称为Flatten，压平后输入特征大小为<span class="math notranslate nohighlight">\(3\times 16 \times 16 = 768\)</span>；
之后做一次全连接对大小为768特征变换到大小为128的特征，再依次做两次全连接分别得到64，10。
这里最后的输出结果是依据自己的实际问题而定，假设我们的输入是包含<span class="math notranslate nohighlight">\(0 \sim 9\)</span>的数字图片，做分类那输出对应是10个概率值，分别对应<span class="math notranslate nohighlight">\(0 \sim 9\)</span>的概率大小。</p>
<div class="figure align-default" id="id25">
<span id="nn-network"></span><a class="reference internal image-reference" href="../_images/nn_network.png"><img alt="../_images/nn_network.png" src="../_images/nn_network.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.6 </span><span class="caption-text">卷积神经网络模型</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p>有了上述基础知识，我们对卷积神经网络所需组件接口和模型构建使用伪代码描述如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># 构建卷积神经网络的组件接口定义：
全连接层接口：fully_connected(input, weights)
卷积层的接口：convolution(input, filters, stride, padding)
最大池化接口：pooling(input, pool_size, stride, padding, mode=&#39;max&#39;)
平均池化接口：pooling(input, pool_size, stride, padding, mode=&#39;mean&#39;)

# 构建卷积神经网络描述：
input:(3,64,64)大小的图片
# 创建卷积模型的训练变量,使用随机数初始化变量值
conv1_filters = variable(random(size=(3, 3, 3, 16)))
conv2_filters = variable(random(size=(3, 3, 16, 32)))
fc1_weights = variable(random(size=(768, 128)))
fc2_weights = variable(random(size=(128, 64)))
fc3_weights = variable(random(size=(64, 10)))
# 将所有需要训练的参数收集起来
all_weights = [conv1_filters, conv2_filters, fc1_weights, fc2_weights, fc3_weights]

# 构建卷积模型的连接过程
output = convolution(input, conv1_filters, stride=1, padding=0)
output = pooling(output, kernel_size=3, stride=1, padding=0, mode=&#39;max&#39;)
output = convolution(output, conv2_filters, stride=1, padding=0)
output = pooling(output, kernel_size=3, stride=1, padding=0, mode=&#39;max&#39;)
output=flatten(output)
output = fully_connected(output, fc1_weights)
output = fully_connected(output, fc2_weights)
output = fully_connected(output, fc3_weights)
</pre></div>
</div>
<p>随着深度神经网络应用领域的扩大，诞生出了丰富的模型构建组件。在卷积神经网络的计算过程中，前后的输入是没有联系的，然而在很多任务中往往需要处理序列信息，如语句、语音、视频等，为了解决此类问题诞生出循环神经网络（Recurrent
Neural Network，RNN）；
循环神经网络很好的解决了序列数据的问题，但是随着序列的增加，长序列又导致了训练过程中梯度消失和梯度爆炸的问题，因此有了长短期记忆（Long
Short-term Memory，LSTM）；
在语言任务中还有Seq2Seq它将RNN当成编解码（Encoder-Decoder）结构的编码器（Encoder）和解码器（Decode）；
在解码器中又常常使用注意力机制（Attention）;基于编解码器和注意力机制又有Transformer；
Transformer又是BERT模型架构的重要组成。随着深度神经网络的发展，未来也会诞生各类模型架构，架构的创新可以通过各类神经网络基本组件的组合来实现。</p>
</div>
<div class="section" id="id14">
<h3><span class="section-number">2.3.2. </span>神经网络层的实现原理<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>2.3.1中使用伪代码定义了一些卷积神经网络接口和模型构建过程，整个构建过程，需要创建训练变量和构建连接过程；
随着网络层数的增加，手动管理训练变量是一个繁琐的过程，因此2.3.1中描述的接口在机器学习库中属于低级API。
机器学习编程库大都提供了更高级用户友好的API，它将神经网络层抽象成一个基类，所有的神经网络层实现都继承基类调用低级API。
如MindSpore提供的mindspore.nn.Cell、mindspore.nn.Conv2d、mindspore.dataset；
PyTorch提供的torch.nn.Module、torch.nn.Conv2d、torch.utils.data.Datset。</p>
<p>图numref:<code class="docutils literal notranslate"><span class="pre">model_build</span></code>描述了神经网络构建过程中的基本细节。
神经网络层需要的功能有该层的训练参数（变量，包括初始化方法和训练状态）以及计算过程；
神经网络模型需要的功能是对神经网络层管理和神经网络层参数的管理。
在机器学习编程库中，承担此功能有MindSpore的Cell、PyTorch的Module。
Cell和Module是模型抽象方法也是所有网络的基类。 现有模型抽象方案有两种。
一种是抽象出两个方法分别为Layer（负责单个神经网络层的参数构建和前向计算），Model（负责对神经网络层进行连接组合和神经网络层参数管理）；
另一种是将Layer和Modle抽象成一个方法，该方法既能表示单层神经网络层也能表示包含多个神经网络层堆叠的模型，Cell和Module就是这样实现的。</p>
<div class="figure align-default" id="id26">
<span id="model-build"></span><a class="reference internal image-reference" href="../_images/model_build.png"><img alt="../_images/model_build.png" src="../_images/model_build.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.7 </span><span class="caption-text">神经网络模型构建细节</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
<p>图numref:<code class="docutils literal notranslate"><span class="pre">cell_abs</span></code>展示了设计神经网络层抽象方法的通用表示。通常在构造器会选择使用Python中collections模块的OrderedDict来初始化神经网络层和神经网络层参数的存储；它的输出是一个有序的，相比与Dict更适合深度学习这种模型堆叠的模式。参数和神经网络层的管理是在__setattr__中实现的，当检测到属性是属于神经网络层及神经网络层参数时就记录起来。神经网络模型比较重要的是计算连接过程，可以在__call__里重载，实现神经网络层时在这里定义计算过程。训练参数的返回接口是为了给优化器传所有训练参数。神经网络层返回为了遍历各层神经网络得到各个神经网络层的参数。这里只列出了一些重要的方法，在自定义方法中，通常需要实现参数插入删除方法、神经网络层插入删除、神经网络模型信息等。</p>
<div class="figure align-default" id="id27">
<span id="cell-abs"></span><a class="reference internal image-reference" href="../_images/cell_abstract.png"><img alt="../_images/cell_abstract.png" src="../_images/cell_abstract.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">图2.3.8 </span><span class="caption-text">神经网络基类抽象方法</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>神经网络接口层基类实现，仅做了简化的描述，在实际实现时，执行计算的__call__方法并不会让用户直接重载，它往往在__call__之外定义一个执行操作的方法（对于神经网络模型该方法是实现网络结构的连接，对于神经网络层则是实现计算过程）后然后在__call__调用；如MindSpore的Cell因为动态图和静态图的执行是不一样的，因此在__call__里定义动态图和计算图的计算执行，在construct方法里定义层或者模型的操作过程。</p>
</div>
<div class="section" id="id15">
<h3><span class="section-number">2.3.3. </span>自定义神经网络层<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>2.3.1中使用伪代码定义机器学习库中低级API，有了实现的神经网络基类抽象方法，那么就可以设计更高层次的接口解决手动管理参数的繁琐。假设已经有了神经网络模型抽象方法Cell，构建Conv2D将继承Cell，并重构__init__和__call__方法，在__init__里初始化训练参数和输入参数，在__call__里调用低级API实现计算逻辑。同样使用伪代码接口描述自定义卷积层的过程。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># 接口定义：
全连接层接口：convolution(input, filters, stride, padding)
变量：Variable(value, trainable=True)
高斯分布初始化方法：random_normal(shape)
神经网络模型抽象方法：Cell

# 定义卷积层
class Conv2D(Cell):
    def __init__(self, in_channels, out_channels, ksize, stride, padding):
        # 卷积核大小为 ksize x ksize x inchannels x out_channels
        filters_shape = (out_channels, in_channels, ksize, ksize)
        self.stride = stride
        self.padding = padding
        self.filters = Variable(random_normal(filters_shape))

    def __call__(self, inputs):
        outputs = convolution(inputs, self.filters, self.stride, self.padding)
</pre></div>
</div>
<p>有了上述定义在使用卷积层时，就不需要创建训练变量了。
如我们需要对<span class="math notranslate nohighlight">\(30 \times 30\)</span>大小10个通道的输入使用<span class="math notranslate nohighlight">\(3 \times 3\)</span>的卷积核做卷积，卷积后输出通道为20调用方式如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">in_channel</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_channel</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">filter_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>其执行过程为，在初始化Conv2D时，__setattr__会判断属性，属于Cell把神经网络层Conv2D记录到self._cells，filters属于parameter把参数记录到self._params。查看神经网络层参数使用conv.parameters_and_names；查看神经网络层列表使用conv.cells_and_names；执行操作使用conv(input)。</p>
</div>
<div class="section" id="id16">
<h3><span class="section-number">2.3.4. </span>自定义神经网络模型<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>神经网络层是Cell的子类（SubClass）实现，同样的神经网络模型也可以采用SubClass的方法自定义神经网络模型；构建时需要在__init__里将要使用的神经网络组件实例化，在__call__里定义神经网络的计算逻辑。同样的以2.3.1的卷积神经网络模型为例，定义接口和伪代码描述如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># 使用Cell子类构建的神经网络层接口定义：
# 构建卷积神经网络的组件接口定义：
全连接层接口：Dense(in_channel, out_channel)
卷积层的接口：Conv2D(in_channel, out_channel, filter_size, stride, padding)
最大池化接口：MaxPool2D(pool_size, stride, padding)
张量平铺：Flatten()

# 使用SubClass方式构建卷积模型
class CNN(Cell):
    def __init__(self):
        self.conv1 = Conv2D(in_channel=3, out_channel=16, filter_size=3, stride=1, padding=0)
        self.maxpool1 = MaxPool2D(pool_size=3, stride=1, padding=0)
        self.conv2 = Conv2D(in_channel=16, out_channel=32, filter_size=3, stride=1, padding=0)
        self.maxpool2 = MaxPool2D(pool_size=3, stride=1, padding=0)
        self.flatten = Flatten()
        self.dense1 = Dense(in_channels=768, out_channel=128)
        self.dense2 = Dense(in_channels=128, out_channel=64)
        self.dense3 = Dense(in_channels=64, out_channel=10)

    def __call__(self, inputs):
        z = self.conv1(inputs)
        z = self.maxpool1(z)
        z = self.conv2(z)
        z = self.maxpool2(z)
        z = self.flatten(z)
        z = self.dense1(z)
        z = self.dense2(z)
        z = self.dense3(z)
        return z
</pre></div>
</div>
<p>上述卷积模型进行实例化，其执行将从__init__开始，第一个是Conv2D，Conv2D也是Cell的子类，会进入到Conv2D的__init__，此时会将第一个Conv2D的卷积参数收集到self._params，之后回到Conv2D，将第一个Conv2D收集到self._cells；第二个的组件是MaxPool2D，因为其没有训练参数，因此将MaxPool2D收集到self._cells；依次类推，分别收集第二个卷积参数和卷积层，三个全连接层的参数和全连接层。实例化之后可以调用.parameters_and_names来返回训练参数；调用用conv.cells_and_names查看神经网络层列表。</p>
</div>
</div>
<div class="section" id="c-c">
<h2><span class="section-number">2.4. </span>C/C++编程接口<a class="headerlink" href="#c-c" title="Permalink to this headline">¶</a></h2>
<p>在上述小节中，我们讨论了开发者如何利用Python来定义机器学习的整个工作流，以及如何定义复杂的深度神经网络。然而，在很多时候，用户也需要添加自定义的算子来帮助实现新的模型，优化器，数据处理函数等。这些自定义算子需要通过C和C++实现，从而获得最优性能。但是为了帮助这些算子被用户使用，他们也需要暴露为Python函数，从而方便用户整合入已有的Python为核心编写的工作流和模型。在这一小节中，我们讨论这一过程是如何实现的。</p>
<div class="section" id="pythonc-c">
<h3><span class="section-number">2.4.1. </span>在Python中调用C/C++函数的原理<a class="headerlink" href="#pythonc-c" title="Permalink to this headline">¶</a></h3>
<p>由于Python的解释器是由C实现的，因此在Python中可以实现对于C和C++函数的调用。现代机器学习框架（包括TensorFlow，PyTorch和MindSpore）主要依赖Pybind11来将底层的大量C和C++函数自动生成对应的Python函数，这一过程一般被称为Python绑定（
Binding）。在Pybind11出现以前，将C和C++函数进行Python绑定的手段主要包括：</p>
<ul class="simple">
<li><p>Python的C-API。这种方式要求在一个C++程序中包含Python.h，并使用Python的C-API对Python语言进行操作。使用这套API需要对Python的底层实现有一定了解，比如如何管理引用计数等，具有较高的使用门槛。</p></li>
<li><p>简单包装界面产生器（Simplified Wrapper and Interface
Generator，SWIG)。SWIG可以将C和C++代码暴露给Python。SWIG是TensorFlow早期使用的方式。这种方式需要用户便携一个复杂的SWIG接口声明文件，并使用SWIG自动生成使用Python
C-API的C代码。自动生成的代码可读性很低，因此具有很大代码维护开销。</p></li>
<li><p>Python的ctypes模块，提供了C语言中的类型，以及直接调用动态链接库的能力。缺点是依赖于C的原生的类型，对自定义类型支持不好。</p></li>
<li><p>CPython是结合了Python和C语言的一种语言，可以简单的认为就是给Python加上了静态类型后的语法，使用者可以维持大部分的Python语法。CPython编写的函数会被自动转译为C和C++代码，因此在CPython中可以插入对于C/C++函数的调用。</p></li>
<li><p>Boost::Python是一个C++库。它可以将C++函数暴露为Python函数。其原理和Python
C-API类似，但是使用方法更简单。然而，由于引入了Boost库，因此有沉重的第三方依赖。</p></li>
</ul>
<p>相对于上述的提供Python绑定的手段，Pybind11提供了类似于Boost::Python的简洁性和易用性，但是其通过专注支持C++
11，并且去除Boost依赖，因此成为了轻量级的Python库，从而特别适合在一个复杂的C++项目（例如本书讨论的机器学习系统）中暴露大量的Python函数。</p>
</div>
<div class="section" id="c">
<h3><span class="section-number">2.4.2. </span>添加C++编写的自定义算子<a class="headerlink" href="#c" title="Permalink to this headline">¶</a></h3>
<p>算子是构建神经网络的基础，在前面也称为低级API；通过算子的封装可以实现各类神经网络层，当开发神经网络层遇到内置算子无法满足时，可以通过自定义算子来实现。以MindSpore为例，实现一个GPU算子需要如下步骤：</p>
<ol class="arabic simple">
<li><p>Primitive注册：算子原语是构建网络模型的基础单元，用户可以直接或者间接调用算子原语搭建一个神经网络模型。</p></li>
<li><p>GPU Kernel实现：GPU Kernel用于调用GPU实现加速计算。</p></li>
<li><p>GPU Kernel注册：算子注册用于将GPU
Kernel及必要信息注册给框架，由框架完成对GPU Kernel的调用。</p></li>
</ol>
<p><strong>1.注册算子原语</strong>
算子原语通常包括算子名、算子输入、算子属性（初始化时需要填的参数，如卷积的stride、padding）、输入数据合法性校验、输出数据类型推导和维度推导。假设需要编写加法算子，主要内容如下：</p>
<ul class="simple">
<li><p>算子名：TensorAdd</p></li>
<li><p>算子属性：构造函数__init__中初始化属性，因加法没有属性，因此__init__不需要额外输入。</p></li>
<li><p>算子输入输出及合法性校验：infer_shape方法中约束两个输入维度必须相同，输出的维度和输入维度相同。infer_dtype方法中约束两个输入数据必须是float32类型，输出的数据类型和输入数据类型相同。
算子输出</p></li>
</ul>
<p>MindSpore中实现注册TensorAdd代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore/ops/operations/math_ops.py</span>
<span class="k">class</span> <span class="nc">TensorAdd</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds two input tensors element-wise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;input dims&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1_shape</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1_shape</span><span class="p">)):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;input_shape&#39;</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x2_shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x1_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_dtype</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x1_dtype&#39;</span><span class="p">:</span> <span class="n">x1_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x2_dtype&#39;</span><span class="p">:</span> <span class="n">x2_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x1_dtype</span>
</pre></div>
</div>
<p>在mindspore/ops/operations/math_ops.py文件内注册加法算子原语后，需要在mindspore/ops/operations/__init__中导出，方便python导入模块时候调用。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore/ops/operations/__init__.py</span>
<span class="kn">from</span> <span class="nn">.math_ops</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Abs</span><span class="p">,</span> <span class="n">ACos</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">TensorAdd</span><span class="p">)</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
  <span class="s1">&#39;ReverseSequence&#39;</span><span class="p">,</span>
  <span class="s1">&#39;CropAndResize&#39;</span><span class="p">,</span>
  <span class="o">...</span><span class="p">,</span>
  <span class="s1">&#39;TensorAdd&#39;</span>
<span class="p">]</span>
</pre></div>
</div>
<p><strong>2.GPU算子开发</strong>继承GPUKernel，实现加法使用类模板定义TensorAddGpuKernel，需要实现以下方法：</p>
<ul class="simple">
<li><p>Init(): 用于完成GPU
Kernel的初始化，通常包括记录算子输入/输出维度，完成Launch前的准备工作；因此在此记录Tensor元素个数。</p></li>
<li><p>GetInputSizeList():向框架反馈输入Tensor需要占用的显存字节数；返回了输入Tensor需要占用的字节数，TensorAdd有两个Input，每个Input占用字节数为element_num<span class="math notranslate nohighlight">\(\ast\)</span>sizeof(T)。</p></li>
<li><p>GetOutputSizeList():向框架反馈输出Tensor需要占用的显存字节数；返回了输出Tensor需要占用的字节数，TensorAdd有一个output，占用element_num<span class="math notranslate nohighlight">\(\ast\)</span>sizeof(T)字节。</p></li>
<li><p>GetWorkspaceSizeList():向框架反馈Workspace字节数，Workspace是用于计算过程中存放临时数据的空间；由于TensorAdd不需要Workspace，因此GetWorkspaceSizeList()返回空的std::vector&lt;size_t&gt;。</p></li>
<li><p>Launch(): 通常调用CUDA kernel(CUDA kernel是基于Nvidia
GPU的并行计算架构开发的核函数)，或者cuDNN接口等方式，完成算子在GPU上加速；Launch()接收input、output在显存的地址，接着调用TensorAdd完成加速。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">mindspore</span><span class="o">/</span><span class="n">ccsrc</span><span class="o">/</span><span class="n">backend</span><span class="o">/</span><span class="n">kernel_compiler</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">math</span><span class="o">/</span><span class="n">tensor_add_v2_gpu_kernel</span><span class="o">.</span><span class="n">h</span>

<span class="n">template</span> <span class="o">&lt;</span><span class="n">typename</span> <span class="n">T</span><span class="o">&gt;</span>
<span class="k">class</span> <span class="nc">TensorAddGpuKernel</span> <span class="p">:</span> <span class="n">public</span> <span class="n">GpuKernel</span> <span class="p">{</span>
 <span class="n">public</span><span class="p">:</span>
  <span class="n">TensorAddGpuKernel</span><span class="p">()</span> <span class="p">:</span> <span class="n">element_num_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
  <span class="o">~</span><span class="n">TensorAddGpuKernel</span><span class="p">()</span> <span class="n">override</span> <span class="o">=</span> <span class="n">default</span><span class="p">;</span>

  <span class="nb">bool</span> <span class="n">Init</span><span class="p">(</span><span class="n">const</span> <span class="n">CNodePtr</span> <span class="o">&amp;</span><span class="n">kernel_node</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
    <span class="n">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">AnfAlgo</span><span class="p">::</span><span class="n">GetPrevNodeOutputInferShape</span><span class="p">(</span><span class="n">kernel_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">shape</span><span class="o">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">element_num_</span> <span class="o">*=</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">InitSizeLists</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">true</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">GetInputSizeList</span><span class="p">()</span> <span class="n">const</span> <span class="n">override</span> <span class="p">{</span> <span class="k">return</span> <span class="n">input_size_list_</span><span class="p">;</span> <span class="p">}</span>
  <span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">GetOutputSizeList</span><span class="p">()</span> <span class="n">const</span> <span class="n">override</span> <span class="p">{</span> <span class="k">return</span> <span class="n">output_size_list_</span><span class="p">;</span> <span class="p">}</span>
  <span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">GetWorkspaceSizeList</span><span class="p">()</span> <span class="n">const</span> <span class="n">override</span> <span class="p">{</span> <span class="k">return</span> <span class="n">workspace_size_list_</span><span class="p">;</span> <span class="p">}</span>

  <span class="nb">bool</span> <span class="n">Launch</span><span class="p">(</span><span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">AddressPtr</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">AddressPtr</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="p">,</span>
              <span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">AddressPtr</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="n">void</span> <span class="o">*</span><span class="n">stream_ptr</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
    <span class="n">T</span> <span class="o">*</span><span class="n">x1</span> <span class="o">=</span> <span class="n">GetDeviceAddress</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">T</span> <span class="o">*</span><span class="n">x2</span> <span class="o">=</span> <span class="n">GetDeviceAddress</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">T</span> <span class="o">*</span><span class="n">y</span> <span class="o">=</span> <span class="n">GetDeviceAddress</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

    <span class="n">TensorAdd</span><span class="p">(</span><span class="n">element_num_</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream_ptr</span><span class="p">));</span>
    <span class="k">return</span> <span class="n">true</span><span class="p">;</span>
  <span class="p">}</span>

 <span class="n">protected</span><span class="p">:</span>
  <span class="n">void</span> <span class="n">InitSizeLists</span><span class="p">()</span> <span class="n">override</span> <span class="p">{</span>
    <span class="n">input_size_list_</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">element_num_</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">));</span>
    <span class="n">input_size_list_</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">element_num_</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">));</span>
    <span class="n">output_size_list_</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">element_num_</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">));</span>
  <span class="p">}</span>

 <span class="n">private</span><span class="p">:</span>
  <span class="n">size_t</span> <span class="n">element_num_</span><span class="p">;</span>
  <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="n">input_size_list_</span><span class="p">;</span>
  <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="n">output_size_list_</span><span class="p">;</span>
  <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="n">workspace_size_list_</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>TensorAdd中调用了CUDA
kernelTensorAddKernel来实现element_num个元素的并行相加:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">mindspore</span><span class="o">/</span><span class="n">ccsrc</span><span class="o">/</span><span class="n">backend</span><span class="o">/</span><span class="n">kernel_compiler</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">math</span><span class="o">/</span><span class="n">tensor_add_v2_gpu_kernel</span><span class="o">.</span><span class="n">h</span>

 <span class="n">template</span> <span class="o">&lt;</span><span class="n">typename</span> <span class="n">T</span><span class="o">&gt;</span>
 <span class="n">__global__</span> <span class="n">void</span> <span class="n">TensorAddKernel</span><span class="p">(</span><span class="n">const</span> <span class="n">size_t</span> <span class="n">element_num</span><span class="p">,</span> <span class="n">const</span> <span class="n">T</span><span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="n">const</span> <span class="n">T</span><span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">T</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">element_num</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="p">}</span>
 <span class="p">}</span>

 <span class="n">template</span> <span class="o">&lt;</span><span class="n">typename</span> <span class="n">T</span><span class="o">&gt;</span>
 <span class="n">void</span> <span class="n">TensorAdd</span><span class="p">(</span><span class="n">const</span> <span class="n">size_t</span> <span class="o">&amp;</span><span class="n">element_num</span><span class="p">,</span> <span class="n">const</span> <span class="n">T</span><span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="n">const</span> <span class="n">T</span><span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">T</span><span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">){</span>
    <span class="n">size_t</span> <span class="n">thread_per_block</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span>
    <span class="n">size_t</span> <span class="n">block_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">element_num</span> <span class="o">+</span> <span class="n">thread_per_block</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">)</span> <span class="o">/</span> <span class="n">thread_per_block</span><span class="p">;</span>
    <span class="n">TensorAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">block_per_grid</span><span class="p">,</span> <span class="n">thread_per_block</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">element_num</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
   <span class="k">return</span><span class="p">;</span>
 <span class="p">}</span>

 <span class="n">template</span> <span class="n">void</span> <span class="n">TensorAdd</span><span class="p">(</span><span class="n">const</span> <span class="n">size_t</span> <span class="o">&amp;</span><span class="n">element_num</span><span class="p">,</span> <span class="n">const</span> <span class="nb">float</span><span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="n">const</span> <span class="nb">float</span><span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="nb">float</span><span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>3.GPU算子注册</strong>算子信息包含1.Primive；2.Input dtype, output
dtype；3.GPU Kernel class； 4.CUDA内置数据类型。框架会根据Primive和Input
dtype, output dtype，调用以CUDA内置数据类型实例化GPU Kernel
class模板类。如下代码中分别注册了支持float和int的TensorAdd算子。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">mindspore</span><span class="o">/</span><span class="n">ccsrc</span><span class="o">/</span><span class="n">backend</span><span class="o">/</span><span class="n">kernel_compiler</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">math</span><span class="o">/</span><span class="n">tensor_add_v2_gpu_kernel</span><span class="o">.</span><span class="n">cc</span>

<span class="n">MS_REG_GPU_KERNEL_ONE</span><span class="p">(</span><span class="n">TensorAddV2</span><span class="p">,</span> <span class="n">KernelAttr</span><span class="p">()</span>
                                    <span class="o">.</span><span class="n">AddInputAttr</span><span class="p">(</span><span class="n">kNumberTypeFloat32</span><span class="p">)</span>
                                    <span class="o">.</span><span class="n">AddInputAttr</span><span class="p">(</span><span class="n">kNumberTypeFloat32</span><span class="p">)</span>
                                    <span class="o">.</span><span class="n">AddOutputAttr</span><span class="p">(</span><span class="n">kNumberTypeFloat32</span><span class="p">),</span>
                      <span class="n">TensorAddV2GpuKernel</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>

<span class="n">MS_REG_GPU_KERNEL_ONE</span><span class="p">(</span><span class="n">TensorAddV2</span><span class="p">,</span> <span class="n">KernelAttr</span><span class="p">()</span>
                                    <span class="o">.</span><span class="n">AddInputAttr</span><span class="p">(</span><span class="n">kNumberTypeInt32</span><span class="p">)</span>
                                    <span class="o">.</span><span class="n">AddInputAttr</span><span class="p">(</span><span class="n">kNumberTypeInt32</span><span class="p">)</span>
                                    <span class="o">.</span><span class="n">AddOutputAttr</span><span class="p">(</span><span class="n">kNumberTypeInt32</span><span class="p">),</span>
                      <span class="n">TensorAddV2GpuKernel</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
<p>完成上述三步工作后，需要把MindSpore重新编译，在源码的根目录执行bash
build.sh -e gpu，最后使用算子进行验证。</p>
</div>
</div>
<div class="section" id="id17">
<h2><span class="section-number">2.5. </span>总结<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>现代机器学习系统需要兼有易用性和高性能，因此其一般选择Python作为前端编程语言，而使用C和C++作为后端编程语言。</p></li>
<li><p>一个机器学习框架需要对一个完整的机器学习应用工作流进行编程支持。这些编程支持一般通过提供高层次Python
API来实现。</p></li>
<li><p>数据处理编程接口允许用户下载，导入和预处理数据集。</p></li>
<li><p>模型定义编程接口允许用户定义和导入机器学习模型。</p></li>
<li><p>损失函数接口允许用户定义损失函数来评估当前模型性能。同时，优化器接口允许用户定义和导入优化算法来基于损失函数计算梯度。</p></li>
<li><p>机器学习框架同时兼有高层次Python
API来对训练过程，模型测试和调试进行支持。</p></li>
<li><p>复杂的深度神经网络可以通过叠加神经网络层来完成。</p></li>
<li><p>用户可以通过Python
API定义神经网络层，并指定神经网络层之间的拓扑来定义深度神经网络。</p></li>
<li><p>Python和C之间的互操作性一般通过CType等技术实现。</p></li>
<li><p>机器学习框架一般具有多种C和C++接口允许用户定义和注册C++实现的算子。这些算子使得用户可以开发高性能模型，数据处理函数，优化器等一系列框架拓展。</p></li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">2. 编程接口</a><ul>
<li><a class="reference internal" href="#id2">2.1. 机器学习系统编程模型的演进</a></li>
<li><a class="reference internal" href="#id3">2.2. 机器学习工作流</a><ul>
<li><a class="reference internal" href="#id4">2.2.1. 环境配置</a></li>
<li><a class="reference internal" href="#id5">2.2.2. 数据处理</a></li>
<li><a class="reference internal" href="#id6">2.2.3. 模型定义</a></li>
<li><a class="reference internal" href="#id7">2.2.4. 损失函数和优化器</a></li>
<li><a class="reference internal" href="#id8">2.2.5. 训练及保存模型</a></li>
<li><a class="reference internal" href="#id9">2.2.6. 测试和验证</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">2.3. 定义深度神经网络</a><ul>
<li><a class="reference internal" href="#id11">2.3.1. 以层为核心定义神经网络</a></li>
<li><a class="reference internal" href="#id14">2.3.2. 神经网络层的实现原理</a></li>
<li><a class="reference internal" href="#id15">2.3.3. 自定义神经网络层</a></li>
<li><a class="reference internal" href="#id16">2.3.4. 自定义神经网络模型</a></li>
</ul>
</li>
<li><a class="reference internal" href="#c-c">2.4. C/C++编程接口</a><ul>
<li><a class="reference internal" href="#pythonc-c">2.4.1. 在Python中调用C/C++函数的原理</a></li>
<li><a class="reference internal" href="#c">2.4.2. 添加C++编写的自定义算子</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id17">2.5. 总结</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../chapter_introduction/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>1. 导论</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_computational_graph/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>3. 计算图</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>